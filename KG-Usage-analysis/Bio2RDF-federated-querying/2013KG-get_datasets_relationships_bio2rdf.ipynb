{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8baf9c1f-2896-4ad7-b778-51eea97c59c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the data\n",
    "df = pd.read_csv('query_features.csv')\n",
    "\n",
    "# Step 2: Filter the DataFrame\n",
    "# Using str.contains to find 'service' in a case-insensitive manner\n",
    "filtered_df = df[df['query'].str.contains('service', case=False, na=False)]\n",
    "\n",
    "# Step 3: Write the selected columns to a new CSV file\n",
    "filtered_df[['query', 'normalized_parse_tree']].to_csv('output.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6083985-f89b-4b57-ad12-b538563639c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "def process_triples(triples, allowed_vocabularies):\n",
    "    triple_counts = defaultdict(int)\n",
    "    # Iterate over each triple, split by newline\n",
    "    for triple in triples.split('\\n'):\n",
    "        parts = triple.strip().split(', ')\n",
    "        # Check if first and last elements contain any of the allowed vocabularies\n",
    "        if len(parts) >= 3 and \\\n",
    "           any(vocab in parts[0] for vocab in allowed_vocabularies) and \\\n",
    "           any(vocab in parts[-1] for vocab in allowed_vocabularies):\n",
    "            if len(parts) == 3:\n",
    "                # For exact triplets\n",
    "                triple_counts[tuple(parts)] += 1\n",
    "            else:\n",
    "                # For longer sequences, create multiple triples\n",
    "                for mid_part in parts[1:-1]:\n",
    "                    triple_counts[(parts[0], mid_part, parts[-1])] += 1\n",
    "    return triple_counts\n",
    "\n",
    "def write_filtered_triples(input_filepath, output_filepath, allowed_vocabularies):\n",
    "    with open(input_filepath, mode='r', newline='', encoding='utf-8') as infile, \\\n",
    "         open(output_filepath, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        \n",
    "        reader = csv.DictReader(infile)\n",
    "        writer = csv.writer(outfile)\n",
    "        \n",
    "        # Write header for the output file\n",
    "        writer.writerow(['subject', 'predicate', 'object', 'count'])\n",
    "        \n",
    "        triple_counts = defaultdict(int)\n",
    "        \n",
    "        # Process each row in the CSV\n",
    "        for row in reader:\n",
    "            triples = row.get('triples', '')\n",
    "            processed_triples = process_triples(triples, allowed_vocabularies)\n",
    "            \n",
    "            # Combine counts from this row with the overall counts\n",
    "            for triple, count in processed_triples.items():\n",
    "                triple_counts[triple] += count\n",
    "\n",
    "        # Write the triples with their counts to the output CSV\n",
    "        for triple, count in triple_counts.items():\n",
    "            writer.writerow(list(triple) + [count])\n",
    "\n",
    "allowed_vocabularies = {\n",
    "    \"sgd\",\n",
    "    \"taxonomy\",\n",
    "    \"homologene\",\n",
    "    \"bioportal\",\n",
    "    \"kegg\",\n",
    "    \"pharmgkb\",\n",
    "    \"hgnc\",\n",
    "    \"omim\",\n",
    "    \"ctd\",\n",
    "    \"drugbank\",\n",
    "    \"mgi\",\n",
    "    \"goa\",\n",
    "    \"wormbase\",\n",
    "    \"affymetrix\",\n",
    "    \"ncbigene\",\n",
    "    \"irefindex\",\n",
    "    \"sider\"\n",
    "}\n",
    "\n",
    "\n",
    "# Specify the path to your input and output files\n",
    "input_filepath = 'entity_predicates.csv'\n",
    "output_filepath = 'federatated_query.csv'\n",
    "\n",
    "# Run the function\n",
    "write_filtered_triples(input_filepath, output_filepath, allowed_vocabularies)\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57248ed8-38e1-4fd7-9626-ff3af05c61a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "def process_triples(triples, allowed_vocabularies):\n",
    "    triple_counts = defaultdict(int)\n",
    "    # Iterate over each triple, split by newline\n",
    "    for triple in triples.split('\\n'):\n",
    "        parts = triple.strip().split(', ')\n",
    "        # Check if first and last elements contain any of the allowed vocabularies\n",
    "        if len(parts) >= 3 and \\\n",
    "           any(vocab in parts[0] for vocab in allowed_vocabularies) and \\\n",
    "           any(vocab in parts[-1] for vocab in allowed_vocabularies):\n",
    "            if len(parts) == 3:\n",
    "                # For exact triplets\n",
    "                triple_counts[tuple(parts)] += 1\n",
    "            else:\n",
    "                # For longer sequences, replace all middle parts with 'var'\n",
    "                triple_counts[(parts[0], 'var', parts[-1])] += 1\n",
    "    return triple_counts\n",
    "\n",
    "def write_filtered_triples(input_filepath, output_filepath, allowed_vocabularies):\n",
    "    with open(input_filepath, mode='r', newline='', encoding='utf-8') as infile, \\\n",
    "         open(output_filepath, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        \n",
    "        reader = csv.DictReader(infile)\n",
    "        writer = csv.writer(outfile)\n",
    "        \n",
    "        # Write header for the output file\n",
    "        writer.writerow(['subject', 'predicate', 'object', 'count'])\n",
    "        \n",
    "        triple_counts = defaultdict(int)\n",
    "        \n",
    "        # Process each row in the CSV\n",
    "        for row in reader:\n",
    "            triples = row.get('triples', '')\n",
    "            processed_triples = process_triples(triples, allowed_vocabularies)\n",
    "            \n",
    "            # Combine counts from this row with the overall counts\n",
    "            for triple, count in processed_triples.items():\n",
    "                # Check if both the subject and object contain 'bio2rdf.org'\n",
    "                if 'bio2rdf.org' in triple[0] and 'bio2rdf.org' in triple[2]:\n",
    "                    triple_counts[triple] += count\n",
    "\n",
    "        # Write the triples with their counts to the output CSV\n",
    "        for triple, count in triple_counts.items():\n",
    "            writer.writerow(list(triple) + [count])\n",
    "\n",
    "allowed_vocabularies = {\n",
    "    \"sgd\",\n",
    "    \"taxonomy\",\n",
    "    \"homologene\",\n",
    "    \"bioportal\",\n",
    "    \"kegg\",\n",
    "    \"pharmgkb\",\n",
    "    \"hgnc\",\n",
    "    \"omim\",\n",
    "    \"ctd\",\n",
    "    \"drugbank\",\n",
    "    \"mgi\",\n",
    "    \"goa\",\n",
    "    \"wormbase\",\n",
    "    \"affymetrix\",\n",
    "    \"ncbigene\",\n",
    "    \"irefindex\",\n",
    "    \"sider\"\n",
    "}\n",
    "\n",
    "\n",
    "# Specify the path to your input and output files\n",
    "input_filepath = 'entity_predicates.csv'\n",
    "output_filepath = 'federatated_query.csv'\n",
    "\n",
    "# Run the function\n",
    "write_filtered_triples(input_filepath, output_filepath, allowed_vocabularies)\n",
    "print(\"done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1ea1bbd-2907-4091-b729-67f1923917bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Define the allowed vocabularies\n",
    "allowed_vocabularies = {\n",
    "    \"sgd\",\n",
    "    \"taxonomy\",\n",
    "    \"homologene\",\n",
    "    \"bioportal\",\n",
    "    \"kegg\",\n",
    "    \"pharmgkb\",\n",
    "    \"hgnc\",\n",
    "    \"omim\",\n",
    "    \"ctd\",\n",
    "    \"drugbank\",\n",
    "    \"mgi\",\n",
    "    \"goa\",\n",
    "    \"wormbase\",\n",
    "    \"affymetrix\",\n",
    "    \"ncbigene\",\n",
    "    \"irefindex\",\n",
    "    \"sider\"\n",
    "}\n",
    "\n",
    "# Load the data from CSV\n",
    "df = pd.read_csv('federatated_query.csv')\n",
    "\n",
    "# Function to extract vocabulary from URI\n",
    "def extract_vocabulary(uri):\n",
    "    for vocab in allowed_vocabularies:\n",
    "        if vocab in uri:\n",
    "            return vocab\n",
    "    return None  # or some default value, e.g., 'unknown'\n",
    "\n",
    "# Apply the function to each relevant column\n",
    "df['subject'] = df['subject'].apply(extract_vocabulary)\n",
    "df['object'] = df['object'].apply(extract_vocabulary)\n",
    "\n",
    "# Simplify the predicate by removing numbers after 'var'\n",
    "df['predicate'] = df['predicate'].apply(lambda x: re.sub(r'.*', 'var', x))\n",
    "\n",
    "# Filter rows where subject and object are not the same\n",
    "df_filtered = df[df['subject'] != df['object']]\n",
    "\n",
    "# Group by subject, predicate, and object, and sum the counts\n",
    "result_df = df_filtered.groupby(['subject', 'predicate', 'object']).sum().reset_index()\n",
    "\n",
    "# Save the modified dataframe to a new CSV\n",
    "result_df.to_csv('2013_query_DS_rel_patterns.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf33eba-94c5-4539-b0b0-29d530f56074",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
