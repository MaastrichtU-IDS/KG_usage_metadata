{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d809e164-7aab-4566-a94b-5f9161a40e31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Step 1: Load the data\n",
    "df = pd.read_csv('query_features.csv')\n",
    "\n",
    "# Step 2: Filter the DataFrame\n",
    "# Using str.contains to find 'service' in a case-insensitive manner\n",
    "filtered_df = df[df['query'].str.contains('service', case=False, na=False)]\n",
    "\n",
    "# Step 3: Write the selected columns to a new CSV file\n",
    "filtered_df[['query', 'normalized_parse_tree']].to_csv('output.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67e84f54-fe21-422d-83f5-90898de1e3bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "def process_triples(triples, allowed_vocabularies):\n",
    "    triple_counts = defaultdict(int)\n",
    "    # Iterate over each triple, split by newline\n",
    "    for triple in triples.split('\\n'):\n",
    "        parts = triple.strip().split(', ')\n",
    "        # Check if first and last elements contain any of the allowed vocabularies\n",
    "        if len(parts) >= 3 and \\\n",
    "           any(vocab in parts[0] for vocab in allowed_vocabularies) and \\\n",
    "           any(vocab in parts[-1] for vocab in allowed_vocabularies):\n",
    "            if len(parts) == 3:\n",
    "                # For exact triplets\n",
    "                triple_counts[tuple(parts)] += 1\n",
    "            else:\n",
    "                # For longer sequences, create multiple triples\n",
    "                for mid_part in parts[1:-1]:\n",
    "                    triple_counts[(parts[0], mid_part, parts[-1])] += 1\n",
    "    return triple_counts\n",
    "\n",
    "def write_filtered_triples(input_filepath, output_filepath, allowed_vocabularies):\n",
    "    with open(input_filepath, mode='r', newline='', encoding='utf-8') as infile, \\\n",
    "         open(output_filepath, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        \n",
    "        reader = csv.DictReader(infile)\n",
    "        writer = csv.writer(outfile)\n",
    "        \n",
    "        # Write header for the output file\n",
    "        writer.writerow(['subject', 'predicate', 'object', 'count'])\n",
    "        \n",
    "        triple_counts = defaultdict(int)\n",
    "        \n",
    "        # Process each row in the CSV\n",
    "        for row in reader:\n",
    "            triples = row.get('triples', '')\n",
    "            processed_triples = process_triples(triples, allowed_vocabularies)\n",
    "            \n",
    "            # Combine counts from this row with the overall counts\n",
    "            for triple, count in processed_triples.items():\n",
    "                triple_counts[triple] += count\n",
    "\n",
    "        # Write the triples with their counts to the output CSV\n",
    "        for triple, count in triple_counts.items():\n",
    "            writer.writerow(list(triple) + [count])\n",
    "\n",
    "allowed_vocabularies = {\n",
    "    'sgd',\n",
    "    'taxonomy',\n",
    "    'homologene',\n",
    "    'interpro',\n",
    "    'bioportal',\n",
    "    'clinicaltrials',\n",
    "    'kegg',\n",
    "    'pharmgkb',\n",
    "    'hgnc',\n",
    "    'mesh',\n",
    "    'omim',\n",
    "    'sider',\n",
    "    'ctd',\n",
    "    'drugbank',\n",
    "    'mgi',\n",
    "    'goa',\n",
    "    'ndc',\n",
    "    'wormbase',\n",
    "    'lsr',\n",
    "    'affymetrix',\n",
    "    'ncbigene',\n",
    "    'irefindex',\n",
    "    'eco',\n",
    "    'hp',\n",
    "    'go',\n",
    "    'apo'}\n",
    "\n",
    "# Specify the path to your input and output files\n",
    "input_filepath = 'entity_predicates.csv'\n",
    "output_filepath = 'federatated_query26.csv'\n",
    "\n",
    "# Run the function\n",
    "write_filtered_triples(input_filepath, output_filepath, allowed_vocabularies)\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e31e5945-3248-45bf-ad17-fdcaa6e298c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Define the allowed vocabularies\n",
    "allowed_vocabularies = {\n",
    "    'sgd', 'taxonomy', 'homologene', 'interpro', 'bioportal', 'clinicaltrials',\n",
    "    'kegg', 'pharmgkb', 'hgnc', 'mesh', 'omim', 'sider', 'ctd', 'drugbank',\n",
    "    'mgi', 'goa', 'ndc', 'wormbase', 'lsr', 'affymetrix', 'ncbigene', 'irefindex', 'eco', \n",
    "    'hp', 'go', 'apo'}\n",
    "\n",
    "# Load the data from CSV\n",
    "df = pd.read_csv('federatated_query26.csv')\n",
    "\n",
    "# Function to extract vocabulary from URI\n",
    "def extract_vocabulary(uri):\n",
    "    for vocab in allowed_vocabularies:\n",
    "        if vocab in uri:\n",
    "            return vocab\n",
    "    return None  # or some default value, e.g., 'unknown'\n",
    "\n",
    "# Apply the function to each relevant column\n",
    "df['subject'] = df['subject'].apply(extract_vocabulary)\n",
    "df['object'] = df['object'].apply(extract_vocabulary)\n",
    "\n",
    "# Simplify the predicate by removing numbers after 'var'\n",
    "df['predicate'] = df['predicate'].apply(lambda x: re.sub(r'var\\d+', 'var', x))\n",
    "\n",
    "# Filter rows where subject and object are not the same\n",
    "df_filtered = df[df['subject'] != df['object']]\n",
    "\n",
    "# Group by subject, predicate, and object, and sum the counts\n",
    "result_df = df_filtered.groupby(['subject', 'predicate', 'object']).sum().reset_index()\n",
    "\n",
    "# Save the modified dataframe to a new CSV\n",
    "result_df.to_csv('2019_query_DS_rel_patterns26.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d16915-6f75-4477-9d76-76b5381779ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
