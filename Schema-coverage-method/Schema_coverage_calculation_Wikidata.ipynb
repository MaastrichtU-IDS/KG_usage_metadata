{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c51abc28",
   "metadata": {},
   "source": [
    "## Dataset preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b41af4e9-df6a-414e-aca5-fc0161819f58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82211742\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>anonymizedQuery\\ttimestamp\\tsourceCategory\\tus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SELECT+*%0AWHERE+%7B%0A++%3Fvar1++%3Chttp%3A%2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SELECT+%3Fvar1++%3Fvar2Label++%3Fvar3++%3Fvar4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>+ASK%0AWHERE+%7B%0A++BIND+%28++YEAR+%28++%3Fva...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SELECT+*%0AWHERE+%7B%0A++%3Fvar1++%3Chttp%3A%2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  anonymizedQuery\\ttimestamp\\tsourceCategory\\tus...\n",
       "1  SELECT+*%0AWHERE+%7B%0A++%3Fvar1++%3Chttp%3A%2...\n",
       "2  SELECT+%3Fvar1++%3Fvar2Label++%3Fvar3++%3Fvar4...\n",
       "3  +ASK%0AWHERE+%7B%0A++BIND+%28++YEAR+%28++%3Fva...\n",
       "4  SELECT+*%0AWHERE+%7B%0A++%3Fvar1++%3Chttp%3A%2..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('2018-02-26_2018-03-25_all.tsv', lineterminator='\\n', dtype=str, header=None)\n",
    "\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c12af16f-12b3-47f5-9d83-dad637a8788f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of robotic queries: 81339186\n",
      "Unique queries and their counts saved to wiki-robotic-unique-count.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Specify the path to your TSV file\n",
    "file_path = '2018-02-26_2018-03-25_all.tsv'  # replace with actual path\n",
    "\n",
    "# Load the file and filter for robotic queries directly\n",
    "df = pd.read_csv(file_path, delimiter='\\t', usecols=['anonymizedQuery', 'sourceCategory'])\n",
    "\n",
    "# Filter for rows where sourceCategory is 'robotic'\n",
    "robotic_queries = df[df['sourceCategory'] == 'robotic']['anonymizedQuery']\n",
    "\n",
    "# Count the total number of robotic queries\n",
    "total_robotic_queries = len(robotic_queries)\n",
    "print(f\"Total number of robotic queries: {total_robotic_queries}\")\n",
    "\n",
    "# Get unique queries and their counts\n",
    "unique_queries_counts = robotic_queries.value_counts()\n",
    "\n",
    "# Write the unique queries and their counts to an output CSV file\n",
    "output_file = 'wiki-robotic-unique-count.csv'  # specify the output path\n",
    "unique_queries_counts.to_csv(output_file, header=['count'], index_label='query')\n",
    "\n",
    "print(f\"Unique queries and their counts saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d5068dd-7478-48e7-a08d-b9d0a5ce9955",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18940103\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SELECT%28++REGEX+%28++%22string1%22%2C+%22stri...</td>\n",
       "      <td>2624171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SELECT+*%0AWHERE+%7B%0A++%3Chttp%3A%2F%2Fwww.w...</td>\n",
       "      <td>804269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>+ASK%0AWHERE+%7B%0A++BIND+%28++%3Chttp%3A%2F%2...</td>\n",
       "      <td>789038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SELECT+%3Fvar1++%3Fvar2+%0AWHERE+%7B%0A++VALUE...</td>\n",
       "      <td>749423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SELECT+%3Fvar1++%3Fvar2++%3Fvar3++%3Fvar4++%3F...</td>\n",
       "      <td>507649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query    count\n",
       "0  SELECT%28++REGEX+%28++%22string1%22%2C+%22stri...  2624171\n",
       "1  SELECT+*%0AWHERE+%7B%0A++%3Chttp%3A%2F%2Fwww.w...   804269\n",
       "2  +ASK%0AWHERE+%7B%0A++BIND+%28++%3Chttp%3A%2F%2...   789038\n",
       "3  SELECT+%3Fvar1++%3Fvar2+%0AWHERE+%7B%0A++VALUE...   749423\n",
       "4  SELECT+%3Fvar1++%3Fvar2++%3Fvar3++%3Fvar4++%3F...   507649"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('wiki-robotic-unique-count.csv')\n",
    "\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3b52b22-b937-454b-a140-2b930cafd2b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded CSV saved as 'decoded_queries.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import urllib.parse\n",
    "\n",
    "# Load the encoded CSV file (replace 'your_file.csv' with your actual file path)\n",
    "df = pd.read_csv('wiki-robotic-unique-count.csv')\n",
    "\n",
    "# Decode the 'query' column\n",
    "df['query'] = df['query'].apply(urllib.parse.unquote_plus)\n",
    "\n",
    "# Save the decoded data to a new CSV file\n",
    "df.to_csv('wiki-robotic-unique-count-decoded_queries2.csv', index=False)\n",
    "\n",
    "print(\"Decoded CSV saved as 'decoded_queries.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f29178c0-01dd-43d1-b1be-5e3294fbbd73",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18940103\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SELECT(  REGEX (  \"string1\", \"string2\" )  AS  ...</td>\n",
       "      <td>2624171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SELECT *\\nWHERE {\\n  &lt;http://www.wikidata.org&gt;...</td>\n",
       "      <td>804269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ASK\\nWHERE {\\n  BIND (  &lt;http://www.wikidata....</td>\n",
       "      <td>789038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SELECT ?var1  ?var2 \\nWHERE {\\n  VALUES (  ?va...</td>\n",
       "      <td>749423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SELECT ?var1  ?var2  ?var3  ?var4  ?var5  ?var...</td>\n",
       "      <td>507649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query    count\n",
       "0  SELECT(  REGEX (  \"string1\", \"string2\" )  AS  ...  2624171\n",
       "1  SELECT *\\nWHERE {\\n  <http://www.wikidata.org>...   804269\n",
       "2   ASK\\nWHERE {\\n  BIND (  <http://www.wikidata....   789038\n",
       "3  SELECT ?var1  ?var2 \\nWHERE {\\n  VALUES (  ?va...   749423\n",
       "4  SELECT ?var1  ?var2  ?var3  ?var4  ?var5  ?var...   507649"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('wiki-robotic-unique-count-decoded_queries2.csv')\n",
    "\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e7c73a1-0ec6-4a40-920f-a1843cc080b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#Prefix Addition \n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def add_prefix(query):\n",
    "    # The common prefix to add to each query\n",
    "    prefix = \"\"\"PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
    "PREFIX xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "PREFIX foaf: <http://xmlns.com/foaf/0.1/>\n",
    "PREFIX dc: <http://purl.org/dc/elements/1.1/>\n",
    "PREFIX dcterms: <http://purl.org/dc/terms/>\n",
    "PREFIX skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "PREFIX schema: <http://schema.org/>\n",
    "PREFIX geo: <http://www.w3.org/2003/01/geo/wgs84_pos#>\n",
    "\n",
    "\"\"\"\n",
    "    return prefix + query\n",
    "\n",
    "def add_prefix_to_csv(input_file, output_file):\n",
    "    # Read the CSV file into a Pandas DataFrame\n",
    "    df = pd.read_csv(input_file)\n",
    "\n",
    "    # Add the prefix to the \"query\" column using the add_prefix function\n",
    "    df['query'] = df['query'].apply(add_prefix)\n",
    "\n",
    "    # Save the updated DataFrame to a new CSV file\n",
    "    df.to_csv(output_file, index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_csv_file = \"wiki-robotic-unique-count-decoded_queries2.csv\"  # Replace with the path to your input CSV file\n",
    "    output_csv_file = \"query_prefixes_added.csv\"  # Replace with the path to your output CSV file\n",
    "    add_prefix_to_csv(input_csv_file, output_csv_file)\n",
    "    print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95df6395-7e49-4f87-be56-9e0b96a260f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18940104\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>query</td>\n",
       "      <td>count</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-...</td>\n",
       "      <td>2624171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-...</td>\n",
       "      <td>804269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-...</td>\n",
       "      <td>789038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PREFIX rdf: &lt;http://www.w3.org/1999/02/22-rdf-...</td>\n",
       "      <td>749423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0        1\n",
       "0                                              query    count\n",
       "1  PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-...  2624171\n",
       "2  PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-...   804269\n",
       "3  PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-...   789038\n",
       "4  PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-...   749423"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('query_prefixes_added.csv', lineterminator='\\n', dtype=str, header=None)\n",
    "\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec4d009-1c50-48ab-aed1-5320345c6fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  parse queries\n",
    "# Install Node.js dependencies\n",
    "!npm install sparqljs csv-parser csv-stringify\n",
    "\n",
    "# Create the JavaScript file\n",
    "js_code = \"\"\"\n",
    "const fs = require('fs');\n",
    "const SparqlParser = require('sparqljs').Parser;\n",
    "const csvParser = require('csv-parser');\n",
    "const { stringify } = require('csv-stringify');\n",
    "\n",
    "const parser = new SparqlParser();\n",
    "\n",
    "async function executeQuery(query) {\n",
    "  try {\n",
    "    const parsedQuery = parser.parse(query);\n",
    "    return JSON.stringify(parsedQuery);\n",
    "  } catch (error) {\n",
    "    console.error('Error parsing query:', query);\n",
    "    return 'Error parsing the query.';\n",
    "  }\n",
    "}\n",
    "\n",
    "async function writeBatchToCsv(batch, outputFile) {\n",
    "  return new Promise((resolve, reject) => {\n",
    "    const writeStream = fs.createWriteStream(outputFile, { flags: 'a' });\n",
    "    const csvStringifier = stringify({ header: false, columns: [ 'Parsed_Query','Count'], delimiter: ',' });\n",
    "\n",
    "    writeStream.on('error', (error) => {\n",
    "      reject(error);\n",
    "    });\n",
    "\n",
    "    csvStringifier.pipe(writeStream);\n",
    "\n",
    "    csvStringifier.on('end', () => {\n",
    "      writeStream.end();\n",
    "      resolve();\n",
    "    });\n",
    "\n",
    "    batch.forEach((entry) => {\n",
    "      csvStringifier.write([ entry.Parsed_Query , entry.Count ]);\n",
    "    });\n",
    "\n",
    "    csvStringifier.end();\n",
    "  });\n",
    "}\n",
    "\n",
    "async function main() {\n",
    "  const inputCsvFile = 'query_prefixes_added.csv';\n",
    "  const outputCsvFile = 'wikidata-robotic-parsed.csv';\n",
    "  const batchSize = 1000000;\n",
    "\n",
    "  const readStream = fs.createReadStream(inputCsvFile).pipe(csvParser());\n",
    "\n",
    "  let batch = [];\n",
    "  for await (const row of readStream) {\n",
    "    const sparqlQuery = row['query'];\n",
    "    const count = row['count'];\n",
    "\n",
    "    const parsedQuery = await executeQuery(sparqlQuery);\n",
    "\n",
    "    batch.push({ Parsed_Query: parsedQuery ,Count: count });\n",
    "\n",
    "    if (batch.length >= batchSize) {\n",
    "      await writeBatchToCsv(batch, outputCsvFile);\n",
    "      console.log(`Processed ${batch.length} queries.`);\n",
    "      batch = [];\n",
    "    }\n",
    "  }\n",
    "\n",
    "  if (batch.length > 0) {\n",
    "    await writeBatchToCsv(batch, outputCsvFile);\n",
    "  }\n",
    "\n",
    "  console.log('All SPARQL queries executed and results written to output CSV file.');\n",
    "}\n",
    "\n",
    "main();\n",
    "\"\"\"\n",
    "\n",
    "# Write JavaScript code to a file\n",
    "with open(\"optimized.js\", \"w\") as f:\n",
    "    f.write(js_code)\n",
    "\n",
    "# Python code to run the JavaScript script\n",
    "python_code = \"\"\"\n",
    "import subprocess\n",
    "\n",
    "def run_script():\n",
    "    # Run the JavaScript code using Node.js with an increased memory limit\n",
    "    result = subprocess.run(['node', '--max-old-space-size=30720', 'optimized.js'], stdout=subprocess.PIPE, text=True)\n",
    "\n",
    "    # Print the output (JSON representation of the parsed SPARQL query)\n",
    "    print(result.stdout)\n",
    "    print('hi')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_script()\n",
    "\"\"\"\n",
    "\n",
    "# Write Python code to a file\n",
    "with open(\"optimized_from_js_.py\", \"w\") as f:\n",
    "    f.write(python_code)\n",
    "\n",
    "# Execute the script directly\n",
    "print(\"Running the script...\")\n",
    "\n",
    "!python optimized_from_js_.py > parseoutput.txt 2>&1\n",
    "\n",
    "# Print the log output\n",
    "with open(\"parseoutput.txt\", \"r\") as log_file:\n",
    "    output = log_file.read()\n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "860315d1-2e6b-435e-8047-b6ff59fd3c9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18940103\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{\"queryType\":\"SELECT\",\"variables\":[{\"expressio...</td>\n",
       "      <td>2624171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{\"queryType\":\"SELECT\",\"variables\":[{}],\"where\"...</td>\n",
       "      <td>804269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{\"queryType\":\"ASK\",\"where\":[{\"type\":\"bind\",\"va...</td>\n",
       "      <td>789038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{\"queryType\":\"SELECT\",\"variables\":[{\"termType\"...</td>\n",
       "      <td>749423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{\"queryType\":\"SELECT\",\"variables\":[{\"termType\"...</td>\n",
       "      <td>507649</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0        1\n",
       "0  {\"queryType\":\"SELECT\",\"variables\":[{\"expressio...  2624171\n",
       "1  {\"queryType\":\"SELECT\",\"variables\":[{}],\"where\"...   804269\n",
       "2  {\"queryType\":\"ASK\",\"where\":[{\"type\":\"bind\",\"va...   789038\n",
       "3  {\"queryType\":\"SELECT\",\"variables\":[{\"termType\"...   749423\n",
       "4  {\"queryType\":\"SELECT\",\"variables\":[{\"termType\"...   507649"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('wikidata-robotic-parsed.csv', lineterminator='\\n', dtype=str, header=None)\n",
    "\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ff98556-a318-4518-883e-85d13555342a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid rows have been written to 'valid_wikidata-robotic-parsed.csv'.\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "# seperate valid queries and unvalid queries\n",
    "\n",
    "import csv\n",
    "\n",
    "# Define the input and output file paths\n",
    "input_file_path = 'wikidata-robotic-parsed.csv'\n",
    "output_file_path = 'valid_wikidata-robotic-parsed.csv'\n",
    "s = 0\n",
    "# Open the input CSV file for reading\n",
    "with open(input_file_path, 'r', newline='\\n', encoding='utf-8') as input_file:\n",
    "    # Create a CSV reader without header\n",
    "    reader = csv.reader(input_file)\n",
    "    \n",
    "    # Create a list to store valid rows\n",
    "    valid_rows = []\n",
    "    \n",
    "    for row in reader:\n",
    "        if row[0] == 'Error parsing the query.':\n",
    "            s += 1\n",
    "        # Check if the second column is not equal to 'Error parsing the query'\n",
    "        elif len(row) >= 2 and row[0] != 'Error parsing the query.':\n",
    "            valid_rows.append(row)\n",
    "\n",
    "# Open the output CSV file for writing\n",
    "with open(output_file_path, 'w', newline='\\n', encoding='utf-8') as output_file:\n",
    "    # Create a CSV writer with column names\n",
    "    writer = csv.writer(output_file)\n",
    "    \n",
    "    # Write column names\n",
    "    writer.writerow(['parsed_query', 'count' ])\n",
    "    \n",
    "    # Write the valid rows\n",
    "    writer.writerows(valid_rows)\n",
    "\n",
    "print(f\"Valid rows have been written to '{output_file_path}'.\")\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e650491-a05e-4106-bf53-f557d9bd93dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18940051\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>parsed_query</td>\n",
       "      <td>count\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{\"queryType\":\"SELECT\",\"variables\":[{\"expressio...</td>\n",
       "      <td>2624171\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{\"queryType\":\"SELECT\",\"variables\":[{}],\"where\"...</td>\n",
       "      <td>804269\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{\"queryType\":\"ASK\",\"where\":[{\"type\":\"bind\",\"va...</td>\n",
       "      <td>789038\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{\"queryType\":\"SELECT\",\"variables\":[{\"termType\"...</td>\n",
       "      <td>749423\\r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0          1\n",
       "0                                       parsed_query    count\\r\n",
       "1  {\"queryType\":\"SELECT\",\"variables\":[{\"expressio...  2624171\\r\n",
       "2  {\"queryType\":\"SELECT\",\"variables\":[{}],\"where\"...   804269\\r\n",
       "3  {\"queryType\":\"ASK\",\"where\":[{\"type\":\"bind\",\"va...   789038\\r\n",
       "4  {\"queryType\":\"SELECT\",\"variables\":[{\"termType\"...   749423\\r"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('valid_wikidata-robotic-parsed.csv', lineterminator='\\n', dtype=str, header=None)\n",
    "\n",
    "print(len(df))\n",
    "df.head()\n",
    "#88494"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c9ad33-df2d-4b69-b61a-9cb49859c7e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#normalize parse tree\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "from multiprocessing import Pool, Lock\n",
    "\n",
    "# Initialize a global lock for writing to the file safely across processes\n",
    "lock = Lock()\n",
    "\n",
    "# The normalization function\n",
    "def find_and_normalize_variables(parsed_query):\n",
    "    unique_vars = set()\n",
    "\n",
    "    def find_variables(node):\n",
    "        if isinstance(node, dict):\n",
    "            for key, value in node.items():\n",
    "                if isinstance(value, dict) or isinstance(value, list):\n",
    "                    find_variables(value)\n",
    "                elif key == \"value\" and node.get(\"termType\") == \"Variable\":\n",
    "                    unique_vars.add(value)\n",
    "        elif isinstance(node, list):\n",
    "            for item in node:\n",
    "                find_variables(item)\n",
    "\n",
    "    def normalize_variables(node, var_mapping):\n",
    "        if isinstance(node, dict):\n",
    "            for key, value in node.items():\n",
    "                if isinstance(value, dict) or isinstance(value, list):\n",
    "                    normalize_variables(value, var_mapping)\n",
    "                elif key == \"value\" and node.get(\"termType\") == \"Variable\":\n",
    "                    node[key] = var_mapping.get(value, value)\n",
    "        elif isinstance(node, list):\n",
    "            for item in node:\n",
    "                normalize_variables(item, var_mapping)\n",
    "\n",
    "    find_variables(parsed_query)\n",
    "    var_mapping = {var: f\"var{index + 1}\" for index, var in enumerate(sorted(unique_vars))}\n",
    "    normalize_variables(parsed_query, var_mapping)\n",
    "\n",
    "    if \"variables\" in parsed_query:\n",
    "        for variable in parsed_query[\"variables\"]:\n",
    "            var_name = variable.get(\"value\")\n",
    "            if var_name in var_mapping:\n",
    "                variable[\"value\"] = var_mapping[var_name]\n",
    "\n",
    "    return parsed_query\n",
    "\n",
    "# Function to apply normalization to each row's parsed query\n",
    "def process_row(row):\n",
    "    parsed_query = json.loads(row['parsed_query'])\n",
    "    normalized_query = find_and_normalize_variables(parsed_query)\n",
    "    return json.dumps(normalized_query)\n",
    "\n",
    "# Function to process a chunk of data, used by each worker process\n",
    "def process_chunk(chunk):\n",
    "    chunk['normalized_parse_tree'] = chunk.apply(process_row, axis=1)\n",
    "    return chunk[['normalized_parse_tree', 'count']]\n",
    "\n",
    "# Function to write a processed chunk to the output file with a lock\n",
    "def write_chunk_to_file(chunk, output_file):\n",
    "    with lock:\n",
    "        chunk.to_csv(output_file, mode='a', index=False, header=False)\n",
    "\n",
    "# Main processing function with parallel processing and file locking\n",
    "def process_large_file_in_chunks(input_file, output_file, chunk_size=50000, num_cpus=30):\n",
    "    # Write the header to the output file initially\n",
    "    pd.DataFrame(columns=['normalized_parse_tree', 'count']).to_csv(output_file, index=False)\n",
    "\n",
    "    # Use multiprocessing pool with specified number of CPUs\n",
    "    with Pool(processes=num_cpus) as pool:\n",
    "        # Read the file in chunks and process each chunk in parallel\n",
    "        for chunk in pd.read_csv(input_file, chunksize=chunk_size):\n",
    "            # Apply `process_chunk` on each chunk asynchronously\n",
    "            result = pool.apply_async(process_chunk, args=(chunk,))\n",
    "            # Write the processed chunk to file once it's ready\n",
    "            write_chunk_to_file(result.get(), output_file)\n",
    "\n",
    "# Run the optimized processing with parallelism and chunking\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = 'valid_wikidata-robotic-parsed.csv'\n",
    "    output_file = 'tree_normalized.csv'\n",
    "    chunk_size = 1000000\n",
    "    num_cpus = 20\n",
    "\n",
    "    process_large_file_in_chunks(input_file, output_file, chunk_size, num_cpus)\n",
    "    print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4c486b-254f-4a0f-b136-0077daa7c05b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('tree_normalized.csv', lineterminator='\\n', dtype=str, header=None)\n",
    "\n",
    "print(len(df))\n",
    "df.head()\n",
    "18940050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0cc5f0-4b59-40e1-9c68-a49338ca350c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool, Lock, cpu_count\n",
    "\n",
    "# File paths\n",
    "input_csv_file = 'tree_normalized.csv'\n",
    "output_csv_file = 'query_features.csv'\n",
    "count_triples_file = 'count_triples.csv'\n",
    "\n",
    "# Initialize a global lock for safe writing to the output file\n",
    "lock = Lock()\n",
    "\n",
    "# Define a recursive function to handle nested path types\n",
    "def extract_predicate_values(items):\n",
    "    predicates = []\n",
    "    for item in items:\n",
    "        if \"value\" in item:\n",
    "            predicates.append(item[\"value\"])\n",
    "        elif \"pathType\" in item:\n",
    "            nested_predicates = extract_predicate_values(item.get(\"items\", []))\n",
    "            predicates.extend(nested_predicates)\n",
    "        else:\n",
    "            predicates.append(\"Unknown\")\n",
    "    return predicates\n",
    "\n",
    "# Modify the extract_triples function to use the recursive function\n",
    "def extract_triples(parse_tree):\n",
    "    local_triples = []\n",
    "    if \"triples\" in parse_tree:\n",
    "        for triple in parse_tree[\"triples\"]:\n",
    "            try:\n",
    "                subject = triple.get(\"subject\", {}).get(\"value\", \"Unknown\")\n",
    "                \n",
    "                # Check if the predicate is a direct value or a path\n",
    "                if \"value\" in triple.get(\"predicate\", {}):\n",
    "                    predicate = triple[\"predicate\"][\"value\"]\n",
    "                elif \"pathType\" in triple.get(\"predicate\", {}):\n",
    "                    items = triple[\"predicate\"].get(\"items\", [])\n",
    "                    predicate_values = extract_predicate_values(items)\n",
    "                    predicate = \", \".join(predicate_values)\n",
    "                else:\n",
    "                    predicate = \"nothing\"\n",
    "                \n",
    "                obj = triple.get(\"object\", {}).get(\"value\", \"Unknown\")\n",
    "                local_triples.append((subject, predicate, obj))\n",
    "            except KeyError as e:\n",
    "                print(f\"Error extracting triple: {e}\")\n",
    "                print(\"Offending triple:\", triple)\n",
    "    \n",
    "    # Recursively call the function on child nodes\n",
    "    for key, value in parse_tree.items():\n",
    "        if isinstance(value, dict):\n",
    "            local_triples += extract_triples(value)\n",
    "        elif isinstance(value, list):\n",
    "            for item in value:\n",
    "                if isinstance(item, dict):\n",
    "                    local_triples += extract_triples(item)\n",
    "    \n",
    "    return local_triples\n",
    "\n",
    "# Process a single row and extract triples\n",
    "def process_row(row, fieldnames):\n",
    "    row_dict = {fieldnames[i]: row[i] for i in range(len(row))}\n",
    "    try:\n",
    "        parse_tree = json.loads(row[0])\n",
    "        triples = extract_triples(parse_tree)\n",
    "        triples_str = '\\n'.join([f\"{s}, {p}, {o}\" for s, p, o in triples])\n",
    "        row_dict['triples'] = triples_str\n",
    "    except json.JSONDecodeError:\n",
    "        row_dict['triples'] = \"Invalid JSON\"\n",
    "    return row_dict\n",
    "\n",
    "# Write processed rows to the output file safely with a lock\n",
    "def write_rows(rows, fieldnames):\n",
    "    with lock:\n",
    "        with open(output_csv_file, 'a', newline='') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "            for row in rows:\n",
    "                writer.writerow(row)\n",
    "\n",
    "# Process a chunk of data, applying process_row to each row\n",
    "def process_chunk(chunk, fieldnames):\n",
    "    processed_rows = [process_row(row, fieldnames) for row in chunk]\n",
    "    write_rows(processed_rows, fieldnames)\n",
    "\n",
    "# Main processing function with parallelism and chunking\n",
    "def process_large_file_in_chunks(input_file, chunk_size=700000, num_cpus=20):\n",
    "    # Get the header and initialize the output file with the header row\n",
    "    with open(input_file, 'r') as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        fieldnames = next(reader) + ['triples']\n",
    "        with open(output_csv_file, 'w', newline='') as outfile:\n",
    "            writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "            writer.writeheader()\n",
    "\n",
    "    # Set up a pool for parallel processing\n",
    "    with Pool(processes=num_cpus) as pool:\n",
    "        chunk = []\n",
    "        with open(input_file, 'r') as infile:\n",
    "            reader = csv.reader(infile)\n",
    "            next(reader)  # Skip header row\n",
    "\n",
    "            # Process each row in chunks\n",
    "            for row in reader:\n",
    "                chunk.append(row)\n",
    "                if len(chunk) >= chunk_size:\n",
    "                    pool.apply_async(process_chunk, args=(chunk, fieldnames))\n",
    "                    chunk = []\n",
    "            \n",
    "            # Process any remaining rows in the last chunk\n",
    "            if chunk:\n",
    "                pool.apply_async(process_chunk, args=(chunk, fieldnames))\n",
    "        \n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "    # Load and inspect the resulting DataFrame\n",
    "    df_input = pd.read_csv(input_file)\n",
    "    print(\"Length of tree_normalized.csv:\", len(df_input))\n",
    "    print(\"Head of tree_normalized.csv:\")\n",
    "    print(df_input.head())\n",
    "\n",
    "    df_output = pd.read_csv(output_csv_file)\n",
    "    print(\"Length of query_features.csv:\", len(df_output))\n",
    "    print(\"Head of query_features.csv:\")\n",
    "    print(df_output.head())\n",
    "\n",
    "    # Calculate unique values in 'normalized_parse_tree'\n",
    "    unique_count = df_input['normalized_parse_tree'].nunique()\n",
    "    print(\"Number of unique values in 'normalized_parse_tree':\", unique_count)\n",
    "\n",
    "    # Select 'count' and 'triples' columns and save to count_triples.csv\n",
    "    selected_columns = df_output[['count', 'triples']]\n",
    "    selected_columns.to_csv(count_triples_file, index=False)\n",
    "    print(\"Selected columns 'count' and 'triples' written to\", count_triples_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_large_file_in_chunks(input_csv_file, chunk_size=700000, num_cpus=20)\n",
    "    print(\"Processing complete. Output written to\", output_csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6edc30e2-47c6-4179-ba12-147e86457834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('query_features.csv', lineterminator='\\n', dtype=str, header=None)\n",
    "\n",
    "print(len(df))\n",
    "df.head()\n",
    "# 844132"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b100fa0-f2e5-4c69-b456-e3c742c33a35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# count and triples \n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Define the path to the input CSV file\n",
    "input_csv_path = 'query_features.csv'  # Make sure to replace this with the actual path to your CSV file\n",
    "\n",
    "# Define the path to the output CSV file\n",
    "output_csv_path = 'count_triples.csv'\n",
    "\n",
    "# Read the input CSV file into a pandas DataFrame\n",
    "df = pd.read_csv(input_csv_path)\n",
    "\n",
    "# Select the 'count' and 'triples' columns\n",
    "selected_columns = df[['count', 'triples']]\n",
    "\n",
    "# Write the selected columns to the output CSV file\n",
    "selected_columns.to_csv(output_csv_path, index=False)\n",
    "\n",
    "print('CSV file \"count_triples.csv\" has been created successfully.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d34234bd-0c61-4bd3-9f72-7f5ba43a2e50",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18940051\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>triples</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2624171</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>804269</td>\n",
       "      <td>http://www.wikidata.org, http://schema.org/dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>789038</td>\n",
       "      <td>var1, http://www.wikidata.org/prop/direct/P279...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>749423</td>\n",
       "      <td>var1, http://www.wikidata.org/prop/direct/P434...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0                                                  1\n",
       "0    count                                            triples\n",
       "1  2624171                                                NaN\n",
       "2   804269  http://www.wikidata.org, http://schema.org/dat...\n",
       "3   789038  var1, http://www.wikidata.org/prop/direct/P279...\n",
       "4   749423  var1, http://www.wikidata.org/prop/direct/P434..."
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('count_triples.csv', lineterminator='\\n', dtype=str, header=None)\n",
    "\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c5bf02-d41a-42ec-987d-b56e374156ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced data with entities and predicates have been saved to entity_predicates.csv\n",
      "Total triples processing errors encountered: 456039\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import csv\n",
    "\n",
    "def refine_list(input_list, exclude_starts_with, exclude_contains):\n",
    "    \"\"\"\n",
    "    Refines the input list by excluding items based on the starting strings or contained strings.\n",
    "    \"\"\"\n",
    "    refined_list = []\n",
    "    for item in input_list:\n",
    "        if any(item.startswith(exclude) for exclude in exclude_starts_with) or any(exclude in item for exclude in exclude_contains):\n",
    "            continue\n",
    "        refined_list.append(item)\n",
    "    return refined_list\n",
    "\n",
    "def process_triples(triples):\n",
    "    errors = 0\n",
    "    entities = []\n",
    "    predicates = []\n",
    "    for triple in triples.split('\\n'):\n",
    "        parts = triple.split(', ')\n",
    "        \n",
    "        # Handling triples with more than three elements\n",
    "        if len(parts) != 3:\n",
    "            if len(parts) > 3 and parts[0] != \"http://www.bigdata.com/rdf#serviceParam\":\n",
    "                entities.append(parts[0])\n",
    "                entities.append(parts[-1])\n",
    "                predicates.extend(parts[1:-1])\n",
    "            elif len(parts) > 3 and parts[0] == \"http://www.bigdata.com/rdf#serviceParam\":\n",
    "                entities.append(parts[0])\n",
    "                predicates.append(parts[1])\n",
    "                entities.extend(parts[2:]) \n",
    "            else:\n",
    "                errors += 1\n",
    "            continue\n",
    "        \n",
    "        entities.extend([parts[0], parts[2]])\n",
    "        predicates.append(parts[1])\n",
    "\n",
    "    exclude_starts_with = ['var', 'e_b', 'g_']\n",
    "    exclude_contains = ['nonsensical']\n",
    "    \n",
    "    # Refining entities and predicates lists\n",
    "    refined_entities = refine_list(entities, exclude_starts_with, exclude_contains)\n",
    "    refined_predicates = refine_list(predicates, exclude_starts_with, exclude_contains)\n",
    "    \n",
    "    return refined_entities, refined_predicates, errors\n",
    "\n",
    "def write_output(input_csv, output_csv):\n",
    "    total_errors = 0\n",
    "    with open(input_csv, mode='r', encoding='utf-8') as infile, \\\n",
    "         open(output_csv, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        fieldnames = reader.fieldnames + ['entities', 'predicates']\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in reader:\n",
    "            entities, predicates, errors = process_triples(row['triples'])\n",
    "            total_errors += errors\n",
    "            row['entities'] = '; '.join(entities)\n",
    "            row['predicates'] = '; '.join(predicates)\n",
    "            writer.writerow(row)\n",
    "    return total_errors\n",
    "\n",
    "# Replace 'input.csv' and 'output.csv' with the actual filenames\n",
    "input_csv = 'count_triples.csv'\n",
    "output_csv = 'entity_predicates.csv'\n",
    "\n",
    "errors_encountered = write_output(input_csv, output_csv)\n",
    "print(\"Enhanced data with entities and predicates have been saved to\", output_csv)\n",
    "print(f\"Total triples processing errors encountered: {errors_encountered}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56e01832-a28c-4f55-9673-5a5e57e36eb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18940051\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>triples</td>\n",
       "      <td>entities</td>\n",
       "      <td>predicates\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2624171</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>804269</td>\n",
       "      <td>http://www.wikidata.org, http://schema.org/dat...</td>\n",
       "      <td>http://www.wikidata.org</td>\n",
       "      <td>http://schema.org/dateModified\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>789038</td>\n",
       "      <td>var1, http://www.wikidata.org/prop/direct/P279...</td>\n",
       "      <td>http://www.bigdata.com/queryHints#Prior; forward</td>\n",
       "      <td>http://www.wikidata.org/prop/direct/P279; http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>749423</td>\n",
       "      <td>var1, http://www.wikidata.org/prop/direct/P434...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://www.wikidata.org/prop/direct/P434\\r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0                                                  1  \\\n",
       "0    count                                            triples   \n",
       "1  2624171                                                NaN   \n",
       "2   804269  http://www.wikidata.org, http://schema.org/dat...   \n",
       "3   789038  var1, http://www.wikidata.org/prop/direct/P279...   \n",
       "4   749423  var1, http://www.wikidata.org/prop/direct/P434...   \n",
       "\n",
       "                                                  2  \\\n",
       "0                                          entities   \n",
       "1                                               NaN   \n",
       "2                           http://www.wikidata.org   \n",
       "3  http://www.bigdata.com/queryHints#Prior; forward   \n",
       "4                                               NaN   \n",
       "\n",
       "                                                   3  \n",
       "0                                       predicates\\r  \n",
       "1                                                 \\r  \n",
       "2                   http://schema.org/dateModified\\r  \n",
       "3  http://www.wikidata.org/prop/direct/P279; http...  \n",
       "4         http://www.wikidata.org/prop/direct/P434\\r  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('entity_predicates.csv', lineterminator='\\n', dtype=str, header=None)\n",
    "\n",
    "print(len(df))\n",
    "df.head()\n",
    "# 844132"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3787fda5-b3db-4805-9911-f835c135c128",
   "metadata": {},
   "source": [
    "# Unique queries\n",
    "\n",
    "## Used schema types\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ed1b785-1973-4c8e-8131-393c7b981986",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated counts written to 8_count_unique_entity_no_repeat.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the input CSV file\n",
    "input_csv = \"entity_predicates.csv\"  # Update this path to your input CSV file\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Initialize a dictionary to hold the aggregated counts\n",
    "aggregated_counts = {}\n",
    "\n",
    "# Iterate through each row of the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Check if the \"entities\" column value is a string\n",
    "    if isinstance(row['entities'], str):\n",
    "        # Split the entities and convert to a set of unique entities\n",
    "        unique_entities = set(row['entities'].strip().split(';'))\n",
    "    else:\n",
    "        # If not a string, proceed with an empty set for this row\n",
    "        unique_entities = set()\n",
    "    \n",
    "    # Simply count each unique entity as one occurrence per row\n",
    "    for entity in unique_entities:\n",
    "        entity = entity.strip()\n",
    "        if entity in aggregated_counts:\n",
    "            aggregated_counts[entity] += 1\n",
    "        else:\n",
    "            aggregated_counts[entity] = 1\n",
    "\n",
    "# Convert the aggregated dictionary to a DataFrame for writing to CSV\n",
    "output_df = pd.DataFrame(list(aggregated_counts.items()), columns=['Entity', 'TotalCount'])\n",
    "\n",
    "# Write the output DataFrame to a CSV file\n",
    "output_csv = \"8_count_unique_entity_no_repeat.csv\"  # Update this path to your output CSV file\n",
    "output_df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Aggregated counts written to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7076baf-bb05-4473-b068-8ea5037940d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12318451\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Entity</td>\n",
       "      <td>TotalCount</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.wikidata.org</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.bigdata.com/queryHints#Prior</td>\n",
       "      <td>37221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>forward</td>\n",
       "      <td>23678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>string1</td>\n",
       "      <td>17796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         0           1\n",
       "0                                   Entity  TotalCount\n",
       "1                  http://www.wikidata.org           1\n",
       "2  http://www.bigdata.com/queryHints#Prior       37221\n",
       "3                                  forward       23678\n",
       "4                                  string1       17796"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('8_count_unique_entity_no_repeat.csv', lineterminator='\\n', dtype=str, header=None)\n",
    "\n",
    "print(len(df))\n",
    "df.head()\n",
    "# 31004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ea78841-6409-40e4-a474-9a9a13e594b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated counts written to heatmap_types.csv\n"
     ]
    }
   ],
   "source": [
    "# extract the types from queries that exist in KG schema\n",
    "\n",
    "import csv\n",
    "\n",
    "# File paths _data/codes/dsri__6_march/journal_paper_codes/wikidata/data\n",
    "input_file_path_1 = '8_count_unique_entity_no_repeat.csv'  # First input file with two columns: type, TotalCount\n",
    "input_file_path_2 = 'clasenahayi.csv'  # Second input file with one column\n",
    "output_file_path = 'robotic_heatmap_unique_types2018_no_repeat.csv'   # Output file\n",
    "\n",
    "# Read the types from the second input file into a set for faster search\n",
    "types_in_second_file = set()\n",
    "with open(input_file_path_2, mode='r', newline='') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader, None)  # Skip header if there is one\n",
    "    for row in reader:\n",
    "        types_in_second_file.add(row[0])\n",
    "\n",
    "# Read the first input file and write relevant rows to the output file\n",
    "with open(input_file_path_1, mode='r', newline='') as infile, \\\n",
    "     open(output_file_path, mode='w', newline='') as outfile:\n",
    "    reader = csv.reader(infile)\n",
    "    writer = csv.writer(outfile)\n",
    "    \n",
    "    header = next(reader)  # Assuming the first row is a header\n",
    "    writer.writerow(header)  # Write the header to the output file\n",
    "    \n",
    "    for row in reader:\n",
    "        \n",
    "        if row[0] in types_in_second_file:  # Check if the type is in the second file\n",
    "            writer.writerow(row)  # Write the whole row to the output file\n",
    "        # else:\n",
    "            # print( row[0])\n",
    "print(\"Aggregated counts written to heatmap_types.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "599a1ab3-66b1-427f-ace2-fcbc6936f91a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58648\n",
      "58648\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entity</th>\n",
       "      <th>TotalCount\\r</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.wikidata.org/entity/Q698</td>\n",
       "      <td>9\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.wikidata.org/entity/Q5</td>\n",
       "      <td>441414\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.wikidata.org/entity/Q16521</td>\n",
       "      <td>1232\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.wikidata.org/entity/Q15981151</td>\n",
       "      <td>15\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.wikidata.org/entity/Q6581072</td>\n",
       "      <td>1833\\r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Entity TotalCount\\r\n",
       "0       http://www.wikidata.org/entity/Q698          9\\r\n",
       "1         http://www.wikidata.org/entity/Q5     441414\\r\n",
       "2     http://www.wikidata.org/entity/Q16521       1232\\r\n",
       "3  http://www.wikidata.org/entity/Q15981151         15\\r\n",
       "4   http://www.wikidata.org/entity/Q6581072       1833\\r"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('robotic_heatmap_unique_types2018_no_repeat.csv', lineterminator='\\n', dtype=str)\n",
    "df_u= df.drop_duplicates()\n",
    "print(len(df))\n",
    "print(len(df_u))\n",
    "df.head()\n",
    "# 12590"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb1553e-7fde-4fbd-a18c-6504b6e117ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "#used Schema predicates based on the unique queries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e4c5f4a-9207-412e-96a8-6a57d90f7175",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated counts written to 8_unique_count_predicates_no_repeat.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the input CSV file\n",
    "input_csv = \"entity_predicates.csv\"  # Update this path to your input CSV file\n",
    "df = pd.read_csv(input_csv)\n",
    "\n",
    "# Initialize a dictionary to hold the aggregated counts\n",
    "aggregated_counts = {}\n",
    "\n",
    "# Iterate through each row of the DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    # Check if the \"entities\" column value is a string\n",
    "    if isinstance(row['predicates'], str):\n",
    "        # Split the entities and convert to a set of unique entities\n",
    "        unique_entities = set(row['predicates'].strip().split(';'))\n",
    "    else:\n",
    "        # If not a string, proceed with an empty set for this row\n",
    "        unique_entities = set()\n",
    "\n",
    "    # Aggregate counts across all rows\n",
    "    for entity in unique_entities:\n",
    "        entity = entity.strip()\n",
    "        if entity in aggregated_counts:\n",
    "            aggregated_counts[entity] += 1\n",
    "        else:\n",
    "            aggregated_counts[entity] = 1\n",
    "\n",
    "# Convert the aggregated dictionary to a DataFrame for writing to CSV\n",
    "output_df = pd.DataFrame(list(aggregated_counts.items()), columns=['predicate', 'TotalCount'])\n",
    "\n",
    "# Write the output DataFrame to a CSV file\n",
    "output_csv = \"8_unique_count_predicates_no_repeat.csv\"  # Update this path to your output CSV file\n",
    "output_df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Aggregated counts written to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37e4fe8e-8dbf-41fa-9e21-857509dee061",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14164\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicate</th>\n",
       "      <th>TotalCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://schema.org/dateModified</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.bigdata.com/queryHints#gearing</td>\n",
       "      <td>37221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.wikidata.org/prop/direct/P279</td>\n",
       "      <td>1064266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.wikidata.org/prop/direct/P434</td>\n",
       "      <td>1100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.wikidata.org/prop/P105</td>\n",
       "      <td>4942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   predicate TotalCount\n",
       "0             http://schema.org/dateModified         59\n",
       "1  http://www.bigdata.com/queryHints#gearing      37221\n",
       "2   http://www.wikidata.org/prop/direct/P279    1064266\n",
       "3   http://www.wikidata.org/prop/direct/P434       1100\n",
       "4          http://www.wikidata.org/prop/P105       4942"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('8_unique_count_predicates_no_repeat.csv', dtype=str)\n",
    "\n",
    "print(len(df))\n",
    "df.head()\n",
    "#1911"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f7b891-ae60-4c0e-ae4c-e5dc741f0076",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path_2 = 'kpredicate.csv'  # Second input file with one column\n",
    "output_file_path = 'heatmap_predicates2017_no_repeat.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc5719fb-36a0-4048-91ec-c12c121dd849",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated counts written to heatmap_predicates77.csv\n"
     ]
    }
   ],
   "source": [
    "# extract valid predicates that exist in schema predicate\n",
    "\n",
    "import csv\n",
    "\n",
    "# File paths\n",
    "input_file_path_1 = '8_unique_count_predicates_no_repeat.csv'  # First input file with two columns: type, TotalCount\n",
    "input_file_path_2 = 'kpredicate.csv'  # Second input file with one column\n",
    "output_file_path = 'robotic_heatmap_unique_predicates2018_no_repeat.csv'   # Output file\n",
    "\n",
    "# Read the types from the second input file into a set for faster search\n",
    "types_in_second_file = set()\n",
    "with open(input_file_path_2, mode='r', newline='') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader, None)  # Skip header if there is one\n",
    "    for row in reader:\n",
    "        types_in_second_file.add(row[0])\n",
    "\n",
    "# Read the first input file and write relevant rows to the output file\n",
    "with open(input_file_path_1, mode='r', newline='') as infile, \\\n",
    "     open(output_file_path, mode='w', newline='') as outfile:\n",
    "    reader = csv.reader(infile)\n",
    "    writer = csv.writer(outfile)\n",
    "    \n",
    "    header = next(reader)  # Assuming the first row is a header\n",
    "    writer.writerow(header)  # Write the header to the output file\n",
    "    \n",
    "    for row in reader:\n",
    "        if row[0] in types_in_second_file:  # Check if the type is in the second file\n",
    "            writer.writerow(row)  # Write the whole row to the output file\n",
    "print(\"Aggregated counts written to heatmap_predicates77.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ac67cf8-6c65-43f1-8d27-ab62d20d2582",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "935\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>predicate</td>\n",
       "      <td>TotalCount\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.wikidata.org/prop/direct/P279</td>\n",
       "      <td>1064266\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.wikidata.org/prop/direct/P17</td>\n",
       "      <td>75600\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.wikidata.org/prop/direct/P21</td>\n",
       "      <td>22934\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.wikidata.org/prop/direct/P19</td>\n",
       "      <td>55728\\r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          0             1\n",
       "0                                 predicate  TotalCount\\r\n",
       "1  http://www.wikidata.org/prop/direct/P279     1064266\\r\n",
       "2   http://www.wikidata.org/prop/direct/P17       75600\\r\n",
       "3   http://www.wikidata.org/prop/direct/P21       22934\\r\n",
       "4   http://www.wikidata.org/prop/direct/P19       55728\\r"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('robotic_heatmap_unique_predicates2018_no_repeat.csv', lineterminator='\\n', dtype=str, header=None)\n",
    "\n",
    "print(len(df))\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
